{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import functools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSCanonical\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rebuild_covariance_matrix(v):\n",
    "    cov = np.zeros((6,6))\n",
    "    cursor = 0\n",
    "    for i in range(6):\n",
    "        cov[i, 0:(i+1)] = v[cursor:(cursor + i + 1)]\n",
    "        cursor += i + 1\n",
    "        \n",
    "    cov = np.dot(cov, cov.T)\n",
    "        \n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_covariance_matrix(c, dims, ax, color='black'):\n",
    "    submatrix = (c[list(dims)])[:,list(dims)]\n",
    "    eigvals, eigvecs = np.linalg.eig(submatrix)\n",
    "    \n",
    "    angle = np.arctan2(eigvecs[0][1], eigvecs[0][0])\n",
    "    \n",
    "    ell = matplotlib.patches.Ellipse((0., 0.), np.sqrt(eigvals[0]), np.sqrt(eigvals[1]), np.rad2deg(angle),\n",
    "                                    linewidth=1.0, edgecolor=color, fill=False)\n",
    "    \n",
    "    ax.add_artist(ell)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_covariance_matrices(c1, c2):\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_covariance_matrix(c1, (0,1), ax)\n",
    "    plot_covariance_matrix(c2, (0,1), ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covariance_matrices_bar_plot(c1, c2, ax):\n",
    "    indices = np.arange(6)\n",
    "    \n",
    "    width = 0.2\n",
    "    ax.bar(indices, np.sqrt(np.diagonal(c1)), width, color='black')\n",
    "    ax.bar(indices + width, np.sqrt(np.diagonal(c2)), width, color='0.6')\n",
    "    ax.set_xticks(indices + width / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(predicted, validation):\n",
    "    return np.mean(np.linalg.norm(predicted - validation, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_validation_set(xs, ys, proportion):\n",
    "    idx = np.arange(len(xs))\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    training_size = int(len(xs) * proportion)\n",
    "\n",
    "    xs_training = xs[idx[0:training_size]]\n",
    "    ys_training = ys[idx[0:training_size]]\n",
    "\n",
    "    xs_validation = xs[idx[training_size:]]\n",
    "    ys_validation = ys[idx[training_size:]]\n",
    "    \n",
    "    return xs_training, ys_training, xs_validation, ys_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(xs, ys, algorithm, n=30):\n",
    "    losses = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        xs_training, ys_training, xs_validation, ys_validation = create_validation_set(xs, ys, 0.7)\n",
    "        \n",
    "        predicted = algorithm(xs_training, ys_training, xs_validation)\n",
    "        \n",
    "        loss = compute_loss(predicted, ys_validation)\n",
    "        losses[i] = loss\n",
    "        \n",
    "    return np.mean(losses), np.std(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = pathlib.Path('/home/dlandry/dataset/recov/tst.json')\n",
    "\n",
    "with dataset_file.open() as f:\n",
    "    dataset_dict = json.load(f)\n",
    "    \n",
    "meta = dataset_dict['metadata']\n",
    "print('{},{},{},{}'.format(dataset_file.name, meta['combiner'], meta['binner'], meta['clustering']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = np.array(dataset_dict['data']['xs'])\n",
    "ys = np.array(dataset_dict['data']['ys'])\n",
    "\n",
    "xs_training, ys_training, xs_validation, ys_validation = create_validation_set(xs,ys,0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pls(xs_training, ys_training, xs_validation, n=3):\n",
    "    pls = PLSCanonical(n_components=n, scale=True)\n",
    "    pls.fit(xs_training, ys_training)\n",
    "    \n",
    "    return pls.predict(xs_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1,5):\n",
    "    partial_pls = functools.partial(pls, n=n)\n",
    "    \n",
    "    result = cross_validation(xs, ys, partial_pls, n=100)\n",
    "    print('Components {}. Loss {}'.format(n, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.neighbors\n",
    "\n",
    "def kd_tree(xs_training, ys_training, xs_validation, k=3):\n",
    "    tree = sklearn.neighbors.KDTree(xs_training)\n",
    "    dist, indices = tree.query(xs_validation, k=k)\n",
    "    \n",
    "    predicted = np.zeros((xs_validation.shape[0], ys_training.shape[1]))\n",
    "    for i in range(len(xs_validation)):\n",
    "        for j in range(k):\n",
    "            ratio = dist[i,j] / np.sum(dist[i])\n",
    "            predicted[i] += ys_training[indices[i,j]] * ratio\n",
    "            \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(1, 15):\n",
    "    kd_tree_partial = functools.partial(kd_tree, k=k)\n",
    "    distribution = cross_validation(xs, ys, kd_tree_partial, n=100)\n",
    "    \n",
    "    print('K {}. Avg Loss {}.'.format(k, distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cca(xs_training, ys_training, xs_validation, n=3):\n",
    "    cca = CCA(n, scale=True)\n",
    "    cca.fit(xs_training, ys_training)\n",
    "    return cca.predict(xs_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1, 6):\n",
    "    cca_partial = functools.partial(cca, n=n)\n",
    "    distribution = cross_validation(xs, ys, cca_partial, n=100)\n",
    "    \n",
    "    print('N {}. Avg Loss {}.'.format(n, distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def support_vector_regression(xs_training, ys_training, xs_validation):\n",
    "    predicted = np.zeros((xs_validation.shape[0], ys_training.shape[1]))\n",
    "    for i in range(ys_training.shape[1]):\n",
    "        svr = SVR()\n",
    "        svr.fit(xs_training, ys_training[:,i])\n",
    "        predicted[:,i] = svr.predict(xs_validation)\n",
    "        \n",
    "    return predicted\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(xs, ys, support_vector_regression, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gp_regression(xs_training, ys_training, xs_validation):\n",
    "    predicted = np.zeros((xs_validation.shape[0], ys_training.shape[1]))\n",
    "    for i in range(ys_training.shape[1]):\n",
    "        gp = GaussianProcessRegressor(normalize_y=True)\n",
    "        gp.fit(xs_training, ys_training[:,i])\n",
    "        predicted[:,i] = gp.predict(xs_validation)\n",
    "        \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(xs,ys,gp_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_regression(xs_training, ys_training, xs_validation, configuration=(100)):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=configuration)\n",
    "    mlp.fit(xs_training, ys_training)\n",
    "    return mlp.predict(xs_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = [\n",
    "#    (100,50,10,50,100),\n",
    "#    (100,100,100,100,100),\n",
    "#    (100),\n",
    "#    (50),\n",
    "#    (200),\n",
    " #   (300),\n",
    "#    (200,100,50,100,200),\n",
    "#    (500, 250, 125, 250, 500),\n",
    "#    (1000, 500, 250, 500, 1000),\n",
    "    (500, 400, 300, 200, 100, 200, 300, 400, 500),\n",
    "    (1000, 800, 600, 400, 200, 400, 600, 80, 1000),\n",
    "#    (500, 250, 125),\n",
    "#    (200, 150, 100, 50)\n",
    "]\n",
    "\n",
    "for configuration in configurations: \n",
    "    partial_mlp_regression = functools.partial(mlp_regression, configuration=configuration)\n",
    "    distribution = cross_validation(xs,ys, partial_mlp_regression)\n",
    "    print('Configuration: {}. Distribution: {}.'.format(configuration, distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hourglass_configuration(min_neurons, max_neurons, n_layers):\n",
    "    configuration = np.zeros(n_layers)\n",
    "    \n",
    "    configuration[0:(n_layers//2)] = np.linspace(max_neurons, min_neurons, num=n_layers//2, endpoint=False)\n",
    "    configuration[n_layers//2] = min_neurons\n",
    "    configuration[(n_layers//2)+1:n_layers] = np.flip(np.linspace(max_neurons, min_neurons, num=(n_layers//2 - 1 + (n_layers % 2)), endpoint=False), axis=0)\n",
    "    \n",
    "    return tuple(configuration.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourglass_configuration(500, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_values = [100, 200, 300, 400, 500]\n",
    "max_values = [250, 500, 750, 1000]\n",
    "n_layers = [3, 5, 7, 9, 11]\n",
    "\n",
    "configs = [(a,b,c) for a in min_values for b in max_values for c in n_layers]\n",
    "\n",
    "for config in configs:\n",
    "    nn_configuration = hourglass_configuration(*config)\n",
    "        \n",
    "    partial_mlp_regression = functools.partial(mlp_regression, configuration=nn_configuration)\n",
    "    distribution = cross_validation(xs,ys, partial_mlp_regression, n=30)\n",
    "    \n",
    "    print('Configuration: {}. Distribution: {}.'.format(nn_configuration, distribution))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs_training, ys_training, xs_validation, ys_validation = create_validation_set(xs, ys, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ys_predicted = mlp_regression(xs_training, ys_training, xs_validation, configuration=(500, 400, 300, 200, 100, 200, 300, 400, 500)) \n",
    "#ys_predicted = kd_tree(xs_training, ys_training, xs_validation, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "to_compare = 18\n",
    "\n",
    "\n",
    "c1 = rebuild_covariance_matrix(ys_validation[to_compare])\n",
    "c2 = rebuild_covariance_matrix(ys_predicted[to_compare])\n",
    "covariance_matrices_bar_plot(c1, c2, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "to_compare = 45\n",
    "\n",
    "plot_covariance_matrix(rebuild_covariance_matrix(ys_validation[to_compare]), (0,1), ax, color='0.0')\n",
    "plot_covariance_matrix(rebuild_covariance_matrix(ys_predicted[to_compare]), (0,1), ax, color='0.5')\n",
    "\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
